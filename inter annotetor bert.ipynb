{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3ba795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-06-16 15:45:06.406316: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Saved figure to file:  visualize.png\n",
      "Figure(1000x1500)\n"
     ]
    }
   ],
   "source": [
    "!bert-score-show --lang en -r \"a top floor of the building with many windows.\" -c \"a lot of people are standing on a green field in front of building.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038499b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (1.4.4)\n",
      "Requirement already satisfied: matplotlib in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (3.6.1)\n",
      "Requirement already satisfied: requests in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (2.28.1)\n",
      "Collecting torch>=1.0.0\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers>=3.0.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from bert-score) (4.64.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2022.1)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.4.0)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->bert-score) (63.4.1)\n",
      "Requirement already satisfied: wheel in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->bert-score) (0.37.1)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.5.post0.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.1/138.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2022.7.9)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from matplotlib->bert-score) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from matplotlib->bert-score) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from matplotlib->bert-score) (4.37.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from matplotlib->bert-score) (0.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->bert-score) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->bert-score) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->bert-score) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->bert-score) (2022.9.24)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.5.post0-py3-none-any.whl size=88255 sha256=d06ea571000b22085cbac617661b295f78f3b7240ce797060ef186cf1fe1594b\n",
      "  Stored in directory: /home1/sm21mtech14004/.cache/pip/wheels/1a/24/92/1e1c9e37be8411a7c7c18a4c54962f5d0a75c56bab4a6f7f57\n",
      "Successfully built lit\n",
      "Installing collected packages: tokenizers, safetensors, mpmath, lit, cmake, sympy, pyyaml, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, fsspec, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, triton, torch, bert-score\n",
      "Successfully installed bert-score-0.3.13 cmake-3.26.4 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.15.1 lit-16.0.5.post0 mpmath-1.3.0 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyyaml-6.0 safetensors-0.3.1 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 transformers-4.30.2 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef53797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:10:09.376575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.903910 R: 0.912246 F1: 0.907919\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/greedy.txt -c ./exp6/act1.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a77d649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:10:21.732225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.903285 R: 0.910405 F1: 0.906716\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/greedy.txt -c ./exp6/act2.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f552fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:10:33.874740: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.905919 R: 0.912489 F1: 0.909066\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/greedy.txt -c ./exp6/act3.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d27e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:10:46.458988: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.916557 R: 0.924423 F1: 0.920341\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/greedy.txt -c ./exp6/act4.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fdbff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:10:58.713143: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.909914 R: 0.913953 F1: 0.911792\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam3.txt -c ./exp6/act1.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f307fa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:11:11.296970: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.909323 R: 0.912144 F1: 0.910630\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam3.txt -c ./exp6/act2.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b775c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:11:23.507916: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.911603 R: 0.913161 F1: 0.912268\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam3.txt -c ./exp6/act3.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9858309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:11:35.605598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.920707 R: 0.923647 F1: 0.922051\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam3.txt -c ./exp6/act4.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69319ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:11:47.799181: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.911336 R: 0.914377 F1: 0.912722\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam5.txt -c ./exp6/act1.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fc9e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:12:00.056383: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.910031 R: 0.911972 F1: 0.910897\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam5.txt -c ./exp6/act2.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f11399de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:12:12.106949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.912484 R: 0.913243 F1: 0.912759\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam5.txt -c ./exp6/act3.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f16be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-18 20:12:24.462455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.2)_fast-tokenizer P: 0.921307 R: 0.923387 F1: 0.922231\n"
     ]
    }
   ],
   "source": [
    "!bert-score -r ./exp6/beam5.txt -c ./exp6/act4.txt --lang en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fc7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526fa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49910464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Similarity: 0.27932352\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_embedding_similarity(caption1, caption2):\n",
    "    # Load the Sentence-BERT model\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "    # Encode the captions\n",
    "    embeddings = model.encode([caption1, caption2])\n",
    "\n",
    "    # Calculate the cosine similarity between the embeddings\n",
    "    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "caption1 = \"The cat is sitting on the mat\"\n",
    "caption2 = \"A tomcat is resting on the floor covering\"\n",
    "similarity_score = calculate_embedding_similarity(caption1, caption2)\n",
    "print(\"Embedding Similarity:\", similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60c36bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f0c6fa944c0>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/20/9c/f07bd70d128fdb107bc02a0c702b9058b4fe147d0ba67b5a0f4c3cf15a54/sentence-transformers-2.2.2.tar.gz\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f0c6fa94760>, 'Connection to files.pythonhosted.org timed out. (connect timeout=15)')': /packages/20/9c/f07bd70d128fdb107bc02a0c702b9058b4fe147d0ba67b5a0f4c3cf15a54/sentence-transformers-2.2.2.tar.gz\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (4.30.2)\n",
      "Requirement already satisfied: tqdm in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (2.0.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (1.23.3)\n",
      "Requirement already satisfied: scikit-learn in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (1.1.2)\n",
      "Requirement already satisfied: scipy in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (1.9.2)\n",
      "Requirement already satisfied: nltk in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sentence_transformers) (0.15.1)\n",
      "Requirement already satisfied: filelock in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: sympy in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (63.4.1)\n",
      "Requirement already satisfied: wheel in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.26.4)\n",
      "Requirement already satisfied: lit in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5.post0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: joblib in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from torchvision->sentence_transformers) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home1/sm21mtech14004/anaconda3/envs/my_env/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=cdb1b181860d828f067a0596855d08587bb862dcb6771da037255145c7c60038\n",
      "  Stored in directory: /home1/sm21mtech14004/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, torchvision, sentence_transformers\n",
      "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99 torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4662b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d0c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3656a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat is sitting on the mat \t\t A tomcat is resting on the floor covering \t\t Score: 0.2874\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = [\"The cat is sitting on the mat\"]\n",
    "\n",
    "sentences2 = [\"A tomcat is resting on the floor covering\"]\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    #print(cosine_scores)\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8646c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "256\n",
      "256\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./exp6/beam5.txt\"\n",
    "\n",
    "# Read the text file and create an array of lines\n",
    "c1 = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        c1.append(line.strip())\n",
    "\n",
    "# Print the array of lines\n",
    "print(len(c1))\n",
    "\n",
    "file_path = \"./exp6/act1.txt\"\n",
    "\n",
    "# Read the text file and create an array of lines\n",
    "c2 = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        c2.append(line.strip())\n",
    "\n",
    "# Print the array of lines\n",
    "print(len(c2))\n",
    "\n",
    "file_path = \"./exp6/act2.txt\"\n",
    "\n",
    "# Read the text file and create an array of lines\n",
    "c3 = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        c3.append(line.strip())\n",
    "\n",
    "# Print the array of lines\n",
    "print(len(c3))\n",
    "\n",
    "file_path = \"./exp6/act3.txt\"\n",
    "\n",
    "# Read the text file and create an array of lines\n",
    "c4 = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        c4.append(line.strip())\n",
    "\n",
    "# Print the array of lines\n",
    "print(len(c4))\n",
    "\n",
    "file_path = \"./exp6/act4.txt\"\n",
    "\n",
    "# Read the text file and create an array of lines\n",
    "c5 = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        c5.append(line.strip())\n",
    "\n",
    "# Print the array of lines\n",
    "print(len(c5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5c052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_sbert(sentences1,sentences2):\n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    \n",
    "    all_sbert = []\n",
    "    \n",
    "    #Output the pairs with their score\n",
    "    for i in range(len(sentences1)):\n",
    "        #print(cosine_scores[i][i])\n",
    "        all_sbert.append(cosine_scores[i][i])\n",
    "        #print(cosine_scores)\n",
    "        #print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))\n",
    "    \n",
    "    #print(all_sbert)\n",
    "    return sum(all_sbert)/len(all_sbert)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "240dae34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 12:  tensor(0.6220, device='cuda:0')\n",
      "Pair 13:  tensor(0.5959, device='cuda:0')\n",
      "Pair 14:  tensor(0.6059, device='cuda:0')\n",
      "Pair 15:  tensor(0.6708, device='cuda:0')\n",
      "result: tensor(0.6237, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x1 = find_sbert(c1,c2)\n",
    "x2 = find_sbert(c1,c3)\n",
    "x3 = find_sbert(c1,c4)\n",
    "x4 = find_sbert(c1,c5)\n",
    "\n",
    "print(\"Pair 12: \",x1)\n",
    "print(\"Pair 13: \",x2)\n",
    "print(\"Pair 14: \",x3)\n",
    "print(\"Pair 15: \",x4)\n",
    "\n",
    "print(\"result:\",(x1+x2+x3+x4)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6922eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Pair 12: \",find_sbert(c1,c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ede28b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
